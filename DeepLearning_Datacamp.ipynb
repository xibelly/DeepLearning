{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CURSO DEEPLEARNIG EN DATACAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4] [4, -5] [2, 7]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_data = np.array([3,5])\n",
    "\n",
    "weights = {'node_0':[2,4], 'node_1':[4,-5], 'output':[2,7]}\n",
    "\n",
    "print(weights['node_0'], weights['node_1'], weights['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-39\n"
     ]
    }
   ],
   "source": [
    "# Calculate node 0 value: node_0_value\n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "\n",
    "# Calculate node 1 value: node_1_value\n",
    "node_1_value = (input_data * weights['node_1']).sum()\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_value, node_1_value])\n",
    "\n",
    "# Calculate output: output\n",
    "output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the ReLU activation fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "    \n",
    "    # Return the value just calculated\n",
    "    return(output)\n",
    "\n",
    "# Calculate node 0 value: node_0_output\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "# Calculate node 1 value: node_1_output\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "# Calculate model output (do not apply relu)\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print model output\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 63, 0, 148]\n"
     ]
    }
   ],
   "source": [
    "# Define predict_with_network()\n",
    "def predict_with_network(input_data_row, weights):\n",
    "\n",
    "    # Calculate node 0 value\n",
    "    node_0_input = (input_data_row * weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    # Calculate node 1 value\n",
    "    node_1_input = (input_data_row * weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_layer_outputs\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Calculate model output\n",
    "    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    # Return model output\n",
    "    return(model_output)\n",
    "\n",
    "\n",
    "# Create empty list to store prediction results\n",
    "input_data = [np.array([3, 5]), np.array([ 1, -1]), np.array([0, 0]), np.array([8, 4])]\n",
    "results = []\n",
    "for input_data_row in input_data:\n",
    "    # Append prediction to results\n",
    "    results.append(predict_with_network(input_data_row, weights))\n",
    "\n",
    "# Print results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the ReLU activation fucntion in a set of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([3,5])\n",
    "weights = {'node_0_0':[2,4], 'node_0_1':[4,-5], 'node_1_0':[-1,2], 'node_1_1':[1,2],'output':[2,7]}\n",
    "\n",
    "def predict_with_network(input_data):\n",
    "    # Calculate node 0 in the first hidden layer\n",
    "    node_0_0_input = (input_data * weights['node_0_0']).sum()\n",
    "    node_0_0_output = relu(node_0_0_input)\n",
    "\n",
    "    # Calculate node 1 in the first hidden layer\n",
    "    node_0_1_input = (input_data * weights['node_0_1']).sum()\n",
    "    node_0_1_output = relu(node_0_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_0_outputs\n",
    "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n",
    "    \n",
    "    # Calculate node 0 in the second hidden layer\n",
    "    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()\n",
    "    node_1_0_output = relu(node_1_0_input)\n",
    "\n",
    "    # Calculate node 1 in the second hidden layer\n",
    "    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()\n",
    "    node_1_1_output = relu(node_1_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_1_outputs\n",
    "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
    "\n",
    "    # Calculate model output: model_output\n",
    "    model_output = (hidden_1_outputs * weights['output']).sum()\n",
    "    \n",
    "    # Return model_output\n",
    "    return(model_output)\n",
    "\n",
    "output = predict_with_network(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_with_network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1cbfaf10f700>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Make prediction using original weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodel_output_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_with_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Calculate error: error_0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_with_network' is not defined"
     ]
    }
   ],
   "source": [
    "# The data point you will make a prediction for\n",
    "input_data = np.array([0, 3])\n",
    "\n",
    "# Sample weights\n",
    "weights_0 = {'node_0': [2, 1],\n",
    "             'node_1': [1, 2],\n",
    "             'output': [1, 1]\n",
    "            }\n",
    "\n",
    "# The actual target value, used to calculate the error\n",
    "target_actual = 3\n",
    "\n",
    "# Make prediction using original weights\n",
    "model_output_0 = predict_with_network(input_data, weights_0)\n",
    "\n",
    "# Calculate error: error_0\n",
    "error_0 = model_output_0 - target_actual\n",
    "\n",
    "# Create weights that cause the network to make perfect prediction (3): weights_1\n",
    "weights_1 = {'node_0': [2, 1],\n",
    "             'node_1': [1, 2],\n",
    "             'output': [1, 0]\n",
    "            }\n",
    "\n",
    "# Make prediction using new weights: model_output_1\n",
    "model_output_1 = predict_with_network(input_data, weights_1)\n",
    "\n",
    "# Calculate error: error_1\n",
    "error_1 = model_output_1 - target_actual\n",
    "\n",
    "# Print error_0 and error_1\n",
    "print(error_0)\n",
    "print(error_1)\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create model_output_0\n",
    "model_output_0 = []\n",
    "# Create model_output_1\n",
    "model_output_1 = []\n",
    "\n",
    "# Loop over input_data\n",
    "for row in input_data:\n",
    "    # Append prediction to model_output_0\n",
    "    model_output_0.append(predict_with_network(row,weights_0))\n",
    "   \n",
    "    # Append prediction to model_output_1\n",
    "    model_output_1.append(predict_with_network(row,weights_1))\n",
    "\n",
    "# Calculate the mean squared error for model_output_0: mse_0\n",
    "mse_0 = mean_squared_error(target_actuals,model_output_0)\n",
    "\n",
    "# Calculate the mean squared error for model_output_1: mse_1\n",
    "mse_1 = mean_squared_error(target_actuals,model_output_1)\n",
    "\n",
    "# Print mse_0 and mse_1\n",
    "print(\"Mean squared error with weights_0: %f\" %mse_0)\n",
    "print(\"Mean squared error with weights_1: %f\" %mse_1)\n",
    "\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating the weights with Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLOPE CALCULATION - NEW WEIGHTS - GRADIENT DESCENT:\n",
    "\n",
    "input_node = 3 , weight= 2, predicted = 6 , target= 10, error = -4\n",
    "\n",
    "*gradient = 2  * (input data) * error\n",
    "\n",
    "* weights_update: weights - learning_rate * gradient -> 2 - 0.01 * (-24) = 2.24  \n",
    "\n",
    "* preds_updated = (weights_update * input_data ).sum()\n",
    "\n",
    "*error_updated =  preds_updated - target\n",
    "\n",
    "# Calculate the predictions: preds\n",
    "preds = (input_data * weights).sum()\n",
    "\n",
    "# Calculate the error: error\n",
    "error = target - preds\n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "# Print the slope\n",
    "print(slope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate: learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Calculate the predictions: preds\n",
    "preds = (weights * input_data).sum()\n",
    "\n",
    "# Calculate the error: error\n",
    "error = preds - target\n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "# Update the weights: weights_updated\n",
    "weights_updated = weights - learning_rate * slope\n",
    "\n",
    "# Get updated predictions: preds_updated\n",
    "preds_updated = (input_data * weights_updated).sum()\n",
    "\n",
    "# Calculate updated error: error_updated\n",
    "error_updated = preds_updated - target\n",
    "\n",
    "# Print the original error\n",
    "print(error)\n",
    "\n",
    "# Print the updated error\n",
    "print(error_updated)\n",
    "\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_updates = 20\n",
    "mse_hist = []\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Iterate over the number of updates\n",
    "for i in range(n_updates):\n",
    "    # Calculate the slope: slope\n",
    "    slope = get_slope(input_data, target, weights)\n",
    "   \n",
    "    # Update the weights: weights\n",
    "    weights = weights - learning_rate * slope\n",
    "   \n",
    "    # Calculate mse with new weights: mse\n",
    "    mse = get_mse(input_data, target, weights)\n",
    "   \n",
    "    # Append the mse to mse_hist\n",
    "    mse_hist.append(mse)\n",
    "\n",
    "# Plot the mse history\n",
    "plt.plot(mse_hist)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a model using KERAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESPECIFYING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPILE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Specify the model\n",
    "n_cols = predictors.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "\n",
    "# Verify that model contains information from compiling\n",
    "print(\"Loss function: \" + model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FITTING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Specify the model\n",
    "n_cols = predictors.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert the target to categorical: target\n",
    "target = to_categorical(df.survived)  #similar to one-hot encoding\n",
    "\n",
    "# Set up the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save, reloading and using a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify, compile, and fit the model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='sgd', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(predictors, target)\n",
    "\n",
    "# Calculate predictions: predictions\n",
    "predictions = model.predict(pred_data)\n",
    "\n",
    "# Calculate predicted probability of survival: predicted_prob_true\n",
    "predicted_prob_true = predictions[:,1] #column corresponding to predicted probabilities \n",
    "\n",
    "# print predicted_prob_true\n",
    "print(predicted_prob_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing optimization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SGD optimizer\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Create list of learning rates: lr_to_test\n",
    "lr_to_test = [.000001, 0.01, 1]\n",
    "\n",
    "# Loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    \n",
    "    # Build new model to test, unaffected by previous models\n",
    "    model = get_new_model()\n",
    "    \n",
    "    # Create SGD optimizer with specified learning rate: my_optimizer\n",
    "    my_optimizer =  SGD(lr=lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=my_optimizer, loss='categorical_crossentropy')\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(predictors,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(predictors, target, validation_split=(0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "early stopping is used to stop optimization when it isn't helping any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import EarlyStopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target, epochs=30, validation_split=(0.3), callbacks=[early_stopping_monitor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_2.add(Dense(100, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(100, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model_1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model_2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding layers to a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input shape to use in the first hidden layer\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first, second, and third hidden layers\n",
    "model_2.add(Dense(50, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(50, activation='relu'))\n",
    "model_2.add(Dense(50, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model 1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model 2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your own digit recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the datasets\n",
    "from keras.datasets import mnist\n",
    "\n",
    "image_size = 28\n",
    "num_images = 60000\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mnist import MNIST\n",
    "\n",
    "#mndata = MNIST('C:/Users/Usuario/OneDrive - Idata/IDATA/Aprendiendo_python/samples/') \n",
    "\n",
    "#images_train, labels_train = mndata.load_training()\n",
    "# or\n",
    "#images, labels = mndata.load_testing()\n",
    "\n",
    "#from mlxtend.data import loadlocal_mnist\n",
    "#X, y = loadlocal_mnist(\n",
    "#        images_path='C:/Users/Usuario/Escritorio/train-images-idx3-ubyte', \n",
    "#        labels_path='C:/Users/Usuario/Escritorio/train-labels-idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the datasets\n",
    "\n",
    "#import gzip\n",
    "#train_img = gzip.open('C:/Users/Usuario/OneDrive - Idata/IDATA/Aprendiendo_python/samples/train-images-idx3-ubyte.gz','r')\n",
    "#train_lab = gzip.open('C:/Users/Usuario/OneDrive - Idata/IDATA/Aprendiendo_python/samples/train-labels-idx1-ubyte.gz','r')\n",
    "\n",
    "#image_size = 28\n",
    "#num_images = 5\n",
    "\n",
    "#train_img.read(16)\n",
    "#buf = train_img.read(image_size * image_size * num_images)\n",
    "#X_train = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "#X_train = X_train.reshape(num_images, image_size, image_size, 1)\n",
    "\n",
    "#train_lab.read(8)\n",
    "#buf2 = train_lab.read(num_images) #image_size * image_size * 1\n",
    "#y_train = np.frombuffer(buf2, dtype=np.uint8).astype(np.int64)\n",
    "#y_train = y_train.reshape(1, image_size*image_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff = train_img.read(image_size * image_size * num_images)\n",
    "#X = np.frombuffer(buff, dtype=np.uint8, count=-1).astype(np.float32)\n",
    "#X_train = X_train.reshape(num_images, image_size, image_size, 1)\n",
    "\n",
    "#buff2 = train_lab.read(1)\n",
    "#y_train = np.frombuffer(buf, dtype=np.uint8, count=-1).astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_lab.read(8)\n",
    "#for i in range(0,50):   \n",
    "#    buf = train_lab.read(1)\n",
    "#    y_train = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "#    print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "X_train size: (60000, 28, 28)\n",
      "y_train size: (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "print(\"X_train size:\",X_train.shape)\n",
    "print(\"y_train size:\",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Displaying the content of X_train\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "image = np.asarray(X_train[4]).squeeze()\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flattening the matrix X\n",
    "def prep_data(X_train,y_train):\n",
    "    out_x=np.zeros((num_images,image_size*image_size))\n",
    "    \n",
    "    out_y = keras.utils.to_categorical(y_train, num_images)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        out_x[i] = X_train[i].reshape(1,image_size*image_size)\n",
    "    return out_x, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size: (60000, 784)\n",
      "y size: (60000, 60000)\n"
     ]
    }
   ],
   "source": [
    "X, y = prep_data(X_train,y_train)\n",
    "print(\"X size:\",X.shape)\n",
    "print(\"y size:\",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/1\n",
      "42000/42000 [==============================] - 445s 11ms/step - loss: 0.3860 - acc: 0.8909 - val_loss: 0.2904 - val_acc: 0.9209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e195d574e0>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50, activation='relu', input_shape=(image_size*image_size, )))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(num_images, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "batch_size = 10000\n",
    "epochs = 6\n",
    "model.fit(X, y, validation_split=0.3) #callbacks=[early_stopping_monitor] batch_size=batch_size, epochs=epochs,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_t size: (10000, 784)\n",
      "y_t size: (10000, 10000)\n",
      "60000/60000 [==============================] - 190s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24598497088526686, 0.92945]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating the model\n",
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test size: (10000, 28, 28)\n",
      "y_test size: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test size:\",X_test.shape)\n",
    "print(\"y_test size:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (10000, 10000) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-239-eb822096a161>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mout_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mX_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_data2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X_t size:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-239-eb822096a161>\u001b[0m in \u001b[0;36mprep_data2\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mout_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mout_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m   \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m   \u001b[0mcategorical\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate array with shape (10000, 10000) and data type float32"
     ]
    }
   ],
   "source": [
    "#Predicting with the model\n",
    "\n",
    "#flattening the matrix X\n",
    "def prep_data2(X,y):\n",
    "    out_x=np.zeros((10000,image_size*image_size))\n",
    "    \n",
    "    out_y = keras.utils.to_categorical(y, 10000)\n",
    "    \n",
    "    for i in range(10000):\n",
    "        out_x[i] = X[i].reshape(1,image_size*image_size)\n",
    "    return out_x, out_y\n",
    "\n",
    "X_t, y_t = prep_data2(X_test,y_test)\n",
    "\n",
    "print(\"X_t size:\",X_t.shape)\n",
    "print(\"y_t size:\",y_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_t size: (10000, 784)\n",
      "y_t size: (10000, 10000)\n",
      "20928/60000 [=========>....................] - ETA: 1:56"
     ]
    }
   ],
   "source": [
    "#Testing the model\n",
    "model.predict(X_t, y_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
